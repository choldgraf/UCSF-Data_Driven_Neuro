{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating `tfrecords` files\n",
    "\n",
    "As we saw in [`001-tf-intro`](001-tf-intro.ipynb), you can use TensorFlow with data that is represented in memory. But for many use-cases, data size is so large that you wouldn't want to read the entire data-set into memory at once. On the other hand, you don't want to spend time in every learning iteration reading data from memory. To solve this conundrum, TensorFlow offers a file-format that integrates neatly with the computation dispatch system underlying . That is, this file-format allows you to read data directly into the computation graph, using specialized functions. \n",
    "\n",
    "We'll see these functions in action in the next notebook. For now, let's stuff some data into `tfrecords` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import skimage.io as sio\n",
    "import os.path as op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata file contains the labels that are associated with the data, and the names of the data files. The classification problem at hand is labeling the images as one of three categories: labeled as 0,1 or 2 in the `label` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(op.join('..', 'data', 'labels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9890.13_80_drn_f_0000_class2.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p2540_98_drn-f_0012_class1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11477.13_96_drn_0005_class1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11477.13_104_drn_final_0034_class1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p2540_98_drn-f_0016_class1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     file  label\n",
       "0        9890.13_80_drn_f_0000_class2.jpg      2\n",
       "1          p2540_98_drn-f_0012_class1.jpg      1\n",
       "2         11477.13_96_drn_0005_class1.jpg      1\n",
       "3  11477.13_104_drn_final_0034_class1.jpg      1\n",
       "4          p2540_98_drn-f_0016_class1.jpg      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains most of the action. We read in the data (label and image) one-by-one from the respective files. \n",
    "We then convert the data into a set of TF \"Features\". These are stored in the file as \"Examples\". The whole thing is serialized and stored in the file. We will deserialize these records as needed -- details in [`003-tf-linear-classifier`](003-tf-linear-classifier.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "      return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "      return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def write_tfrecords(labels_df, fname, idx):\n",
    "    writer = tf.python_io.TFRecordWriter(fname)\n",
    "    for example_idx in idx:\n",
    "        # Read in the data one-by-one:\n",
    "        image = sio.imread(op.join('../data/cells/', \n",
    "                                   labels_df['file'][example_idx]))\n",
    "        label = labels_df['label'][example_idx]\n",
    "        rows = image.shape[0]\n",
    "        cols = image.shape[1]\n",
    "        depth = image.shape[2]\n",
    "        image_raw = image.tostring()\n",
    "        # construct the Example proto object\n",
    "        example = tf.train.Example(\n",
    "            # Example contains a Features proto object\n",
    "            features=tf.train.Features(feature={\n",
    "            # Features contains a map of string to Feature proto objects\n",
    "                'image/height': _int64_feature(rows),\n",
    "                'image/width': _int64_feature(cols), \n",
    "                'image/depth': _int64_feature(depth),\n",
    "                'label': _int64_feature(int(label)),\n",
    "                'image/raw': _bytes_feature(image_raw)}))\n",
    "                \n",
    "        # use the proto object to serialize the example to a string\n",
    "        serialized = example.SerializeToString()\n",
    "        # write the serialized object to disk\n",
    "        writer.write(serialized)\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a set of indices to use to select an example each time. The data has to be randomly shuffled in advance, to take advantage of TF's out-of-core shuffling mechanisms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = np.arange(labels_df.shape[0])\n",
    "np.random.shuffle(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into three sets: the first is used for training, the second is used for evaluating the training procedure, while it is still taking place. The third is used as a test set, to evaluate the whole procedure at its end. See Chapter 7 of Hastie, Tibshirani and Friedman's [\"Elements of Statistical Learning\"]( http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prop_train = 0.6\n",
    "prop_eval = 0.2 \n",
    "# First 60% are for training:\n",
    "train_idx = idx[:int(prop_train*idx.shape[0])]\n",
    "# Next 20% are for evaluation:\n",
    "eval_idx = idx[int(prop_train*idx.shape[0]):int(prop_train*idx.shape[0] + prop_eval*idx.shape[0])]\n",
    "# Last 20% are for testing:\n",
    "test_idx = idx[int(prop_train*idx.shape[0] + prop_eval*idx.shape[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "tfrecords_train_file = op.join('../data', 'cells_train.tfrecords')\n",
    "tfrecords_eval_file = op.join('../data', 'cells_eval.tfrecords')\n",
    "tfrecords_test_file = op.join('../data', 'cells_test.tfrecords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_tfrecords(labels_df, tfrecords_train_file, train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_tfrecords(labels_df, tfrecords_eval_file, eval_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_tfrecords(labels_df, tfrecords_test_file, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
